{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention  , self).__init__()\n",
    "    \n",
    "    def call(self , query , key , value , mask = None):\n",
    "        matmul_qk = tf.matmul(query , key , transpose_b= True)\n",
    "        dk  = tf.cast(tf.shape(key)[-1] , dtype = tf.float32)\n",
    "        scaled_attendtion_logits = matmul_qk / tf.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            scaled_attendtion_logits += (mask * -1e9)\n",
    "        attention_weights = tf.nn.softmax(scaled_attendtion_logits)\n",
    "        context_vector = tf.matmul(attention_weights , value)\n",
    "        \n",
    "        return context_vector , attention_weights \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the attention \n",
    "\n",
    "query = tf.random.normal([1,60 , 512]) # [batch_size , sequence_length , hidden dims]\n",
    "key = tf.random.normal([1, 60, 512])\n",
    "value = tf.random.normal([1, 60 , 512])\n",
    "\n",
    "attention_layer = ScaledDotProductAttention()\n",
    "context_vector  , attention_weights = attention_layer(query , key ,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights shape :  (1, 60, 60)\n",
      "context_vector shape :  (1, 60, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"attention_weights shape : \",attention_weights.shape)\n",
    "print(\"context_vector shape : \",context_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 60, 512), dtype=float32, numpy=\n",
       " array([[[ 0.17700657,  0.181385  , -0.20478751, ...,  0.09377327,\n",
       "           0.06309219,  0.06153682],\n",
       "         [ 0.03401825,  0.12424516, -0.09543878, ...,  0.3135886 ,\n",
       "           0.11113255, -0.11701623],\n",
       "         [ 0.04679409,  0.0247407 ,  0.26106104, ...,  0.23319082,\n",
       "           0.14586933, -0.08070141],\n",
       "         ...,\n",
       "         [ 0.37619588, -0.1041597 ,  0.22569837, ...,  0.30400464,\n",
       "          -0.12813361,  0.25159466],\n",
       "         [ 0.15836282,  0.12380538, -0.18803303, ...,  0.03463506,\n",
       "           0.32412484,  0.01808381],\n",
       "         [ 0.15389511,  0.19485563,  0.0048784 , ...,  0.12644738,\n",
       "           0.20467182, -0.07748351]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 60, 60), dtype=float32, numpy=\n",
       " array([[[0.02331792, 0.0091478 , 0.00264895, ..., 0.00545185,\n",
       "          0.00654283, 0.00580621],\n",
       "         [0.00313364, 0.03895336, 0.00517009, ..., 0.00870177,\n",
       "          0.01769357, 0.0627395 ],\n",
       "         [0.01095345, 0.00731473, 0.00600484, ..., 0.0018625 ,\n",
       "          0.06699592, 0.00470417],\n",
       "         ...,\n",
       "         [0.00104593, 0.00049879, 0.02223082, ..., 0.00307262,\n",
       "          0.02924519, 0.0089897 ],\n",
       "         [0.00431808, 0.02171337, 0.0344287 , ..., 0.04102536,\n",
       "          0.04213981, 0.04877738],\n",
       "         [0.00121915, 0.01066433, 0.00351683, ..., 0.00626448,\n",
       "          0.00594422, 0.00305349]]], dtype=float32)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector , attention_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
